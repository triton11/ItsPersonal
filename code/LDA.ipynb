{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YO\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import unicodedata\n",
    "import string\n",
    "import csv\n",
    "import random\n",
    "\n",
    "all_data = []\n",
    "def read_lines():\n",
    "\twith open('../data/train_data.csv') as csv_file:\n",
    "\t\tcsv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\t\tline_count = 0\n",
    "\t\tprint(\"YO\")\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif (line_count != 0):\n",
    "\t\t\t\tall_data.append(row)\n",
    "\t\t\tline_count += 1\n",
    "lines = read_lines()\n",
    "corpus = [item[1] for item in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Tristrum/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "processed_docs = [preprocess(i) for i in corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.002*\"tapatalk\" + 0.001*\"send\" + 0.001*\"dear\" + 0.001*\"hello\" + 0.001*\"tongu\" + 0.001*\"women\" + 0.001*\"fuck\" + 0.001*\"intuit\" + 0.001*\"dont\" + 0.001*\"femal\"\n",
      "Topic: 1 Word: 0.002*\"dont\" + 0.002*\"tapatalk\" + 0.001*\"send\" + 0.001*\"hello\" + 0.001*\"fuck\" + 0.001*\"descript\" + 0.001*\"enneagram\" + 0.001*\"cognit\" + 0.001*\"kinda\" + 0.001*\"valu\"\n",
      "Topic: 2 Word: 0.001*\"dont\" + 0.001*\"fuck\" + 0.001*\"dear\" + 0.001*\"result\" + 0.001*\"intuit\" + 0.001*\"annoy\" + 0.001*\"hurt\" + 0.001*\"logic\" + 0.001*\"stupid\" + 0.001*\"colleg\"\n",
      "Topic: 3 Word: 0.001*\"dont\" + 0.001*\"tongu\" + 0.001*\"appreci\" + 0.001*\"fuck\" + 0.001*\"enneagram\" + 0.001*\"develop\" + 0.001*\"hello\" + 0.001*\"logic\" + 0.001*\"domin\" + 0.001*\"sister\"\n",
      "Topic: 4 Word: 0.001*\"fuck\" + 0.001*\"okay\" + 0.001*\"boyfriend\" + 0.001*\"discuss\" + 0.001*\"charact\" + 0.001*\"difficult\" + 0.001*\"annoy\" + 0.001*\"draw\" + 0.001*\"valu\" + 0.001*\"math\"\n",
      "Topic: 5 Word: 0.001*\"fuck\" + 0.001*\"shit\" + 0.001*\"women\" + 0.001*\"logic\" + 0.001*\"hello\" + 0.001*\"comment\" + 0.001*\"boyfriend\" + 0.001*\"extrovert\" + 0.001*\"joke\" + 0.001*\"valu\"\n",
      "Topic: 6 Word: 0.001*\"dont\" + 0.001*\"fuck\" + 0.001*\"depress\" + 0.001*\"proud\" + 0.001*\"extrovert\" + 0.001*\"hurt\" + 0.001*\"enneagram\" + 0.001*\"perc\" + 0.001*\"drink\" + 0.001*\"beauti\"\n",
      "Topic: 7 Word: 0.002*\"tapatalk\" + 0.002*\"send\" + 0.001*\"fuck\" + 0.001*\"dont\" + 0.001*\"tongu\" + 0.001*\"iphon\" + 0.001*\"trust\" + 0.001*\"drink\" + 0.001*\"repli\" + 0.001*\"charact\"\n",
      "Topic: 8 Word: 0.001*\"tongu\" + 0.001*\"domin\" + 0.001*\"score\" + 0.001*\"valu\" + 0.001*\"theori\" + 0.001*\"trust\" + 0.001*\"kitteh\" + 0.001*\"femal\" + 0.001*\"beauti\" + 0.001*\"drink\"\n",
      "Topic: 9 Word: 0.001*\"dont\" + 0.001*\"sister\" + 0.001*\"enneagram\" + 0.001*\"hello\" + 0.001*\"beauti\" + 0.001*\"sleep\" + 0.001*\"studi\" + 0.001*\"inform\" + 0.001*\"intuit\" + 0.001*\"femal\"\n",
      "Topic: 10 Word: 0.001*\"dont\" + 0.001*\"anim\" + 0.001*\"hello\" + 0.001*\"femal\" + 0.001*\"depress\" + 0.001*\"wear\" + 0.001*\"extrovert\" + 0.001*\"intuit\" + 0.001*\"fuck\" + 0.001*\"intellig\"\n",
      "Topic: 11 Word: 0.001*\"dont\" + 0.001*\"hello\" + 0.001*\"logic\" + 0.001*\"theori\" + 0.001*\"discuss\" + 0.001*\"charact\" + 0.001*\"femal\" + 0.001*\"smile\" + 0.001*\"sleep\" + 0.001*\"drink\"\n",
      "Topic: 12 Word: 0.001*\"domin\" + 0.001*\"fuck\" + 0.001*\"connect\" + 0.001*\"level\" + 0.001*\"extrovert\" + 0.001*\"pictur\" + 0.001*\"stereotyp\" + 0.001*\"dear\" + 0.001*\"parent\" + 0.001*\"appreci\"\n",
      "Topic: 13 Word: 0.001*\"tongu\" + 0.001*\"dont\" + 0.001*\"perc\" + 0.001*\"hello\" + 0.001*\"okay\" + 0.001*\"enneagram\" + 0.001*\"mother\" + 0.001*\"kinda\" + 0.001*\"sleep\" + 0.001*\"anim\"\n",
      "Topic: 14 Word: 0.002*\"tmlt\" + 0.002*\"dont\" + 0.001*\"that\" + 0.001*\"tongu\" + 0.001*\"male\" + 0.001*\"hello\" + 0.001*\"connect\" + 0.001*\"charact\" + 0.001*\"sleep\" + 0.001*\"smile\"\n",
      "Topic: 15 Word: 0.002*\"dont\" + 0.001*\"femal\" + 0.001*\"tongu\" + 0.001*\"repli\" + 0.001*\"topic\" + 0.001*\"class\" + 0.001*\"parent\" + 0.001*\"dear\" + 0.001*\"shit\" + 0.001*\"colleg\"\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.25, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=16, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = \"my_new_LDA_model\"\n",
    "lda_model_tfidf.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.002*\"tapatalk\" + 0.001*\"send\" + 0.001*\"dear\" + 0.001*\"hello\" + 0.001*\"tongu\" + 0.001*\"women\" + 0.001*\"fuck\" + 0.001*\"intuit\" + 0.001*\"dont\" + 0.001*\"femal\"\n",
      "Topic: 1 Word: 0.002*\"dont\" + 0.002*\"tapatalk\" + 0.001*\"send\" + 0.001*\"hello\" + 0.001*\"fuck\" + 0.001*\"descript\" + 0.001*\"enneagram\" + 0.001*\"cognit\" + 0.001*\"kinda\" + 0.001*\"valu\"\n",
      "Topic: 2 Word: 0.001*\"dont\" + 0.001*\"fuck\" + 0.001*\"dear\" + 0.001*\"result\" + 0.001*\"intuit\" + 0.001*\"annoy\" + 0.001*\"hurt\" + 0.001*\"logic\" + 0.001*\"stupid\" + 0.001*\"colleg\"\n",
      "Topic: 3 Word: 0.001*\"dont\" + 0.001*\"tongu\" + 0.001*\"appreci\" + 0.001*\"fuck\" + 0.001*\"enneagram\" + 0.001*\"develop\" + 0.001*\"hello\" + 0.001*\"logic\" + 0.001*\"domin\" + 0.001*\"sister\"\n",
      "Topic: 4 Word: 0.001*\"fuck\" + 0.001*\"okay\" + 0.001*\"boyfriend\" + 0.001*\"discuss\" + 0.001*\"charact\" + 0.001*\"difficult\" + 0.001*\"annoy\" + 0.001*\"draw\" + 0.001*\"valu\" + 0.001*\"math\"\n",
      "Topic: 5 Word: 0.001*\"fuck\" + 0.001*\"shit\" + 0.001*\"women\" + 0.001*\"logic\" + 0.001*\"hello\" + 0.001*\"comment\" + 0.001*\"boyfriend\" + 0.001*\"extrovert\" + 0.001*\"joke\" + 0.001*\"valu\"\n",
      "Topic: 6 Word: 0.001*\"dont\" + 0.001*\"fuck\" + 0.001*\"depress\" + 0.001*\"proud\" + 0.001*\"extrovert\" + 0.001*\"hurt\" + 0.001*\"enneagram\" + 0.001*\"perc\" + 0.001*\"drink\" + 0.001*\"beauti\"\n",
      "Topic: 7 Word: 0.002*\"tapatalk\" + 0.002*\"send\" + 0.001*\"fuck\" + 0.001*\"dont\" + 0.001*\"tongu\" + 0.001*\"iphon\" + 0.001*\"trust\" + 0.001*\"drink\" + 0.001*\"repli\" + 0.001*\"charact\"\n",
      "Topic: 8 Word: 0.001*\"tongu\" + 0.001*\"domin\" + 0.001*\"score\" + 0.001*\"valu\" + 0.001*\"theori\" + 0.001*\"trust\" + 0.001*\"kitteh\" + 0.001*\"femal\" + 0.001*\"beauti\" + 0.001*\"drink\"\n",
      "Topic: 9 Word: 0.001*\"dont\" + 0.001*\"sister\" + 0.001*\"enneagram\" + 0.001*\"hello\" + 0.001*\"beauti\" + 0.001*\"sleep\" + 0.001*\"studi\" + 0.001*\"inform\" + 0.001*\"intuit\" + 0.001*\"femal\"\n",
      "Topic: 10 Word: 0.001*\"dont\" + 0.001*\"anim\" + 0.001*\"hello\" + 0.001*\"femal\" + 0.001*\"depress\" + 0.001*\"wear\" + 0.001*\"extrovert\" + 0.001*\"intuit\" + 0.001*\"fuck\" + 0.001*\"intellig\"\n",
      "Topic: 11 Word: 0.001*\"dont\" + 0.001*\"hello\" + 0.001*\"logic\" + 0.001*\"theori\" + 0.001*\"discuss\" + 0.001*\"charact\" + 0.001*\"femal\" + 0.001*\"smile\" + 0.001*\"sleep\" + 0.001*\"drink\"\n",
      "Topic: 12 Word: 0.001*\"domin\" + 0.001*\"fuck\" + 0.001*\"connect\" + 0.001*\"level\" + 0.001*\"extrovert\" + 0.001*\"pictur\" + 0.001*\"stereotyp\" + 0.001*\"dear\" + 0.001*\"parent\" + 0.001*\"appreci\"\n",
      "Topic: 13 Word: 0.001*\"tongu\" + 0.001*\"dont\" + 0.001*\"perc\" + 0.001*\"hello\" + 0.001*\"okay\" + 0.001*\"enneagram\" + 0.001*\"mother\" + 0.001*\"kinda\" + 0.001*\"sleep\" + 0.001*\"anim\"\n",
      "Topic: 14 Word: 0.002*\"tmlt\" + 0.002*\"dont\" + 0.001*\"that\" + 0.001*\"tongu\" + 0.001*\"male\" + 0.001*\"hello\" + 0.001*\"connect\" + 0.001*\"charact\" + 0.001*\"sleep\" + 0.001*\"smile\"\n",
      "Topic: 15 Word: 0.002*\"dont\" + 0.001*\"femal\" + 0.001*\"tongu\" + 0.001*\"repli\" + 0.001*\"topic\" + 0.001*\"class\" + 0.001*\"parent\" + 0.001*\"dear\" + 0.001*\"shit\" + 0.001*\"colleg\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "# data = datapath(\"my_LDA_model\")\n",
    "model_test = gensim.models.LdaMulticore.load(\"my_new_LDA_model\")\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YO\n"
     ]
    }
   ],
   "source": [
    "all_data_test = []\n",
    "\n",
    "def read_lines_test():\n",
    "\twith open('../data/test_data.csv') as csv_file:\n",
    "\t\tcsv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\t\tline_count = 0\n",
    "\t\tprint(\"YO\")\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif (line_count != 0):\n",
    "\t\t\t\tall_data_test.append(row)\n",
    "\t\t\tline_count += 1\n",
    "read_lines_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('lda_results_train.csv', mode='w') as write_file:\n",
    "    employee_writer = csv.writer(write_file, delimiter=',')\n",
    "    employee_writer.writerow(['mbti', 'text', 'sadness', 'joy', 'fear', 'disgust', 'anger', 'topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14', 'topic15'])\n",
    "    for row in all_data:\n",
    "        doc = row[1]\n",
    "        processed_test_doc = preprocess(doc)\n",
    "        clean_doc = dictionary.doc2bow(processed_test_doc)\n",
    "    #     v = corpus_tfidf[clean_docs]\n",
    "        vect = lda_model_tfidf[clean_doc]\n",
    "        new_row = []\n",
    "        new_row.append(int(row[0]))\n",
    "        new_row.append(row[1])\n",
    "        new_row.append(float(row[2]))\n",
    "        new_row.append(float(row[3]))\n",
    "        new_row.append(float(row[4]))\n",
    "        new_row.append(float(row[5]))\n",
    "        new_row.append(float(row[6]))\n",
    "        all_scores = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        for topic_index, score in vect:\n",
    "            all_scores[topic_index] = score\n",
    "        new_row += all_scores\n",
    "        employee_writer.writerow(new_row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('lda_results_test.csv', mode='w') as write_file:\n",
    "    employee_writer = csv.writer(write_file, delimiter=',')\n",
    "    employee_writer.writerow(['mbti', 'text', 'sadness', 'joy', 'fear', 'disgust', 'anger', 'topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14', 'topic15'])\n",
    "    for row in all_data_test:\n",
    "        doc = row[1]\n",
    "        processed_test_doc = preprocess(doc)\n",
    "        clean_doc = dictionary.doc2bow(processed_test_doc)\n",
    "    #     v = corpus_tfidf[clean_docs]\n",
    "        vect = lda_model_tfidf[clean_doc]\n",
    "        new_row = []\n",
    "        new_row.append(int(row[0]))\n",
    "        new_row.append(row[1])\n",
    "        new_row.append(float(row[2]))\n",
    "        new_row.append(float(row[3]))\n",
    "        new_row.append(float(row[4]))\n",
    "        new_row.append(float(row[5]))\n",
    "        new_row.append(float(row[6]))\n",
    "        all_scores = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        for topic_index, score in vect:\n",
    "            all_scores[topic_index] = score\n",
    "        new_row += all_scores\n",
    "        employee_writer.writerow(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = \"tfidf_LDA_model\"\n",
    "lda_model_tfidf.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
