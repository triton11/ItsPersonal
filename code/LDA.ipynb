{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YO\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import unicodedata\n",
    "import string\n",
    "import csv\n",
    "import random\n",
    "\n",
    "all_data = []\n",
    "def read_lines():\n",
    "\twith open('../data/train_data.csv') as csv_file:\n",
    "\t\tcsv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\t\tline_count = 0\n",
    "\t\tprint(\"YO\")\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif (line_count != 0):\n",
    "\t\t\t\tall_data.append(row)\n",
    "\t\t\tline_count += 1\n",
    "lines = read_lines()\n",
    "corpus = [item[1] for item in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Tristrum/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "processed_docs = [preprocess(i) for i in corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.001*\"result\" + 0.001*\"sleep\" + 0.001*\"hello\" + 0.001*\"class\" + 0.001*\"interview\" + 0.001*\"femal\" + 0.001*\"okay\" + 0.001*\"valu\" + 0.001*\"joke\" + 0.001*\"sister\"\n",
      "Topic: 1 Word: 0.002*\"dear\" + 0.001*\"dont\" + 0.001*\"fear\" + 0.001*\"beauti\" + 0.001*\"hurt\" + 0.001*\"myer\" + 0.001*\"brigg\" + 0.001*\"kinda\" + 0.001*\"food\" + 0.001*\"fuck\"\n",
      "Topic: 2 Word: 0.001*\"blah\" + 0.001*\"dont\" + 0.001*\"boyfriend\" + 0.001*\"sleep\" + 0.001*\"extrovert\" + 0.001*\"strang\" + 0.001*\"tongu\" + 0.001*\"wors\" + 0.001*\"class\" + 0.001*\"descript\"\n",
      "Topic: 3 Word: 0.001*\"dear\" + 0.001*\"fuck\" + 0.001*\"smile\" + 0.001*\"cute\" + 0.001*\"dont\" + 0.001*\"text\" + 0.001*\"state\" + 0.001*\"theori\" + 0.001*\"black\" + 0.001*\"socion\"\n",
      "Topic: 4 Word: 0.001*\"dont\" + 0.001*\"extrovert\" + 0.001*\"husband\" + 0.001*\"religion\" + 0.001*\"marri\" + 0.001*\"intuit\" + 0.001*\"money\" + 0.001*\"fuck\" + 0.001*\"math\" + 0.001*\"studi\"\n",
      "Topic: 5 Word: 0.002*\"dont\" + 0.001*\"charact\" + 0.001*\"enneagram\" + 0.001*\"commit\" + 0.001*\"marri\" + 0.001*\"topic\" + 0.001*\"instinct\" + 0.001*\"theori\" + 0.001*\"women\" + 0.001*\"half\"\n",
      "Topic: 6 Word: 0.001*\"eye\" + 0.001*\"beauti\" + 0.001*\"parent\" + 0.001*\"dont\" + 0.001*\"sim\" + 0.001*\"develop\" + 0.001*\"control\" + 0.001*\"walk\" + 0.001*\"lack\" + 0.001*\"super\"\n",
      "Topic: 7 Word: 0.001*\"send\" + 0.001*\"dont\" + 0.001*\"shit\" + 0.001*\"anim\" + 0.001*\"fuck\" + 0.001*\"note\" + 0.001*\"appreci\" + 0.001*\"tapatalk\" + 0.001*\"topic\" + 0.001*\"engin\"\n",
      "Topic: 8 Word: 0.001*\"hello\" + 0.001*\"intuit\" + 0.001*\"mother\" + 0.001*\"femal\" + 0.001*\"logic\" + 0.001*\"tongu\" + 0.001*\"connect\" + 0.001*\"suggest\" + 0.001*\"send\" + 0.001*\"fear\"\n",
      "Topic: 9 Word: 0.001*\"dont\" + 0.001*\"dear\" + 0.001*\"hello\" + 0.001*\"depress\" + 0.001*\"tapatalk\" + 0.001*\"enneagram\" + 0.001*\"brain\" + 0.001*\"women\" + 0.001*\"hell\" + 0.001*\"charact\"\n",
      "Topic: 10 Word: 0.001*\"quot\" + 0.001*\"fuck\" + 0.001*\"hurt\" + 0.001*\"number\" + 0.001*\"brother\" + 0.001*\"annoy\" + 0.001*\"depress\" + 0.001*\"difficult\" + 0.001*\"control\" + 0.001*\"rare\"\n",
      "Topic: 11 Word: 0.001*\"dont\" + 0.001*\"hurt\" + 0.001*\"result\" + 0.001*\"tongu\" + 0.001*\"kitteh\" + 0.001*\"stupid\" + 0.001*\"marriag\" + 0.001*\"parent\" + 0.001*\"studi\" + 0.001*\"brother\"\n",
      "Topic: 12 Word: 0.001*\"hello\" + 0.001*\"dont\" + 0.001*\"advic\" + 0.001*\"rock\" + 0.001*\"passion\" + 0.001*\"join\" + 0.001*\"sleep\" + 0.001*\"onlin\" + 0.001*\"rare\" + 0.001*\"perc\"\n",
      "Topic: 13 Word: 0.001*\"enneagram\" + 0.001*\"tongu\" + 0.001*\"femal\" + 0.001*\"fuck\" + 0.001*\"dont\" + 0.001*\"girlfriend\" + 0.001*\"obvious\" + 0.001*\"hello\" + 0.001*\"kitteh\" + 0.001*\"studi\"\n",
      "Topic: 14 Word: 0.001*\"fuck\" + 0.001*\"cri\" + 0.001*\"enneagram\" + 0.001*\"realiti\" + 0.001*\"brother\" + 0.001*\"dont\" + 0.001*\"accept\" + 0.001*\"okay\" + 0.001*\"blue\" + 0.001*\"hurt\"\n",
      "Topic: 15 Word: 0.002*\"tapatalk\" + 0.002*\"send\" + 0.001*\"iphon\" + 0.001*\"valu\" + 0.001*\"dont\" + 0.001*\"bodi\" + 0.001*\"light\" + 0.001*\"fuck\" + 0.001*\"charact\" + 0.001*\"socion\"\n",
      "Topic: 16 Word: 0.003*\"tmlt\" + 0.001*\"boyfriend\" + 0.001*\"beauti\" + 0.001*\"drink\" + 0.001*\"proud\" + 0.001*\"cognit\" + 0.001*\"hair\" + 0.001*\"tapatalk\" + 0.001*\"cute\" + 0.001*\"studi\"\n",
      "Topic: 17 Word: 0.001*\"tongu\" + 0.001*\"lack\" + 0.001*\"shit\" + 0.001*\"curious\" + 0.001*\"color\" + 0.001*\"easi\" + 0.001*\"women\" + 0.001*\"drink\" + 0.001*\"awar\" + 0.001*\"control\"\n",
      "Topic: 18 Word: 0.002*\"dont\" + 0.001*\"dear\" + 0.001*\"fuck\" + 0.001*\"femal\" + 0.001*\"hug\" + 0.001*\"perc\" + 0.001*\"hello\" + 0.001*\"sleep\" + 0.001*\"brother\" + 0.001*\"grade\"\n",
      "Topic: 19 Word: 0.001*\"shit\" + 0.001*\"teacher\" + 0.001*\"perc\" + 0.001*\"math\" + 0.001*\"research\" + 0.001*\"fuck\" + 0.001*\"kinda\" + 0.001*\"necessari\" + 0.001*\"annoy\" + 0.001*\"engin\"\n",
      "Topic: 20 Word: 0.001*\"dont\" + 0.001*\"hello\" + 0.001*\"kinda\" + 0.001*\"afraid\" + 0.001*\"enneagram\" + 0.001*\"male\" + 0.001*\"debat\" + 0.001*\"tongu\" + 0.001*\"dead\" + 0.001*\"constant\"\n",
      "Topic: 21 Word: 0.001*\"dont\" + 0.001*\"fiction\" + 0.001*\"exist\" + 0.001*\"worri\" + 0.001*\"studi\" + 0.001*\"kill\" + 0.001*\"score\" + 0.001*\"soul\" + 0.001*\"clear\" + 0.001*\"shit\"\n",
      "Topic: 22 Word: 0.001*\"fuck\" + 0.001*\"intuit\" + 0.001*\"dear\" + 0.001*\"drink\" + 0.001*\"brain\" + 0.001*\"shit\" + 0.001*\"okay\" + 0.001*\"dont\" + 0.001*\"choos\" + 0.001*\"questionnair\"\n",
      "Topic: 23 Word: 0.001*\"enneagram\" + 0.001*\"dont\" + 0.001*\"shit\" + 0.001*\"anim\" + 0.001*\"film\" + 0.001*\"studi\" + 0.001*\"descript\" + 0.001*\"okay\" + 0.001*\"star\" + 0.001*\"countri\"\n",
      "Topic: 24 Word: 0.001*\"dont\" + 0.001*\"heart\" + 0.001*\"quot\" + 0.001*\"enneagram\" + 0.001*\"stand\" + 0.001*\"fuck\" + 0.001*\"clear\" + 0.001*\"tongu\" + 0.001*\"anyway\" + 0.001*\"final\"\n",
      "Topic: 25 Word: 0.002*\"socion\" + 0.001*\"okay\" + 0.001*\"theori\" + 0.001*\"couldn\" + 0.001*\"tongu\" + 0.001*\"anim\" + 0.001*\"hello\" + 0.001*\"sister\" + 0.001*\"anxieti\" + 0.001*\"normal\"\n",
      "Topic: 26 Word: 0.001*\"femal\" + 0.001*\"repli\" + 0.001*\"male\" + 0.001*\"shit\" + 0.001*\"beauti\" + 0.001*\"obsess\" + 0.001*\"control\" + 0.001*\"gonna\" + 0.001*\"women\" + 0.001*\"hello\"\n",
      "Topic: 27 Word: 0.001*\"fuck\" + 0.001*\"dont\" + 0.001*\"danc\" + 0.001*\"studi\" + 0.001*\"food\" + 0.001*\"metal\" + 0.001*\"cognit\" + 0.001*\"brain\" + 0.001*\"class\" + 0.001*\"insid\"\n",
      "Topic: 28 Word: 0.001*\"enneagram\" + 0.001*\"tmlt\" + 0.001*\"tongu\" + 0.001*\"cognit\" + 0.001*\"extrovert\" + 0.001*\"blush\" + 0.001*\"wink\" + 0.001*\"dont\" + 0.001*\"parent\" + 0.001*\"fall\"\n",
      "Topic: 29 Word: 0.001*\"intuit\" + 0.001*\"enneagram\" + 0.001*\"anim\" + 0.001*\"exist\" + 0.001*\"theori\" + 0.001*\"perc\" + 0.001*\"evil\" + 0.001*\"logic\" + 0.001*\"allow\" + 0.001*\"avoid\"\n",
      "Topic: 30 Word: 0.001*\"dont\" + 0.001*\"hello\" + 0.001*\"logic\" + 0.001*\"repli\" + 0.001*\"shit\" + 0.001*\"english\" + 0.001*\"kinda\" + 0.001*\"fuck\" + 0.001*\"tongu\" + 0.001*\"wink\"\n",
      "Topic: 31 Word: 0.001*\"charact\" + 0.001*\"beauti\" + 0.001*\"proud\" + 0.001*\"inferior\" + 0.001*\"anim\" + 0.001*\"send\" + 0.001*\"fear\" + 0.001*\"truli\" + 0.001*\"ravenclaw\" + 0.001*\"tongu\"\n",
      "Topic: 32 Word: 0.002*\"dont\" + 0.002*\"youtub\" + 0.001*\"tongu\" + 0.001*\"seri\" + 0.001*\"dear\" + 0.001*\"lead\" + 0.001*\"kinda\" + 0.001*\"hello\" + 0.001*\"trityp\" + 0.001*\"kitteh\"\n",
      "Topic: 33 Word: 0.001*\"dont\" + 0.001*\"fuck\" + 0.001*\"enneagram\" + 0.001*\"depress\" + 0.001*\"babi\" + 0.001*\"proud\" + 0.001*\"brother\" + 0.001*\"okay\" + 0.001*\"domin\" + 0.001*\"anim\"\n",
      "Topic: 34 Word: 0.001*\"tongu\" + 0.001*\"dont\" + 0.001*\"clear\" + 0.001*\"ignor\" + 0.001*\"brother\" + 0.001*\"hello\" + 0.001*\"state\" + 0.001*\"studi\" + 0.001*\"way\" + 0.001*\"blush\"\n",
      "Topic: 35 Word: 0.002*\"dont\" + 0.001*\"flickr\" + 0.001*\"fuck\" + 0.001*\"extrovert\" + 0.001*\"scienc\" + 0.001*\"colleg\" + 0.001*\"class\" + 0.001*\"tongu\" + 0.001*\"shit\" + 0.001*\"sociopath\"\n",
      "Topic: 36 Word: 0.001*\"dont\" + 0.001*\"sleep\" + 0.001*\"drink\" + 0.001*\"math\" + 0.001*\"kinda\" + 0.001*\"shock\" + 0.001*\"beauti\" + 0.001*\"bear\" + 0.001*\"result\" + 0.001*\"hello\"\n",
      "Topic: 37 Word: 0.005*\"tapatalk\" + 0.004*\"iphon\" + 0.003*\"send\" + 0.001*\"dont\" + 0.001*\"drink\" + 0.001*\"intuit\" + 0.001*\"sister\" + 0.001*\"rule\" + 0.001*\"pictur\" + 0.001*\"wink\"\n",
      "Topic: 38 Word: 0.002*\"dont\" + 0.001*\"drink\" + 0.001*\"daughter\" + 0.001*\"bear\" + 0.001*\"hug\" + 0.001*\"respect\" + 0.001*\"logic\" + 0.001*\"mother\" + 0.001*\"gift\" + 0.001*\"discuss\"\n",
      "Topic: 39 Word: 0.001*\"dont\" + 0.001*\"critic\" + 0.001*\"scienc\" + 0.001*\"fuck\" + 0.001*\"charact\" + 0.001*\"connect\" + 0.001*\"femal\" + 0.001*\"child\" + 0.001*\"topic\" + 0.001*\"intellig\"\n",
      "Topic: 40 Word: 0.001*\"dont\" + 0.001*\"women\" + 0.001*\"sister\" + 0.001*\"trust\" + 0.001*\"depress\" + 0.001*\"tongu\" + 0.001*\"appreci\" + 0.001*\"domin\" + 0.001*\"result\" + 0.001*\"descript\"\n",
      "Topic: 41 Word: 0.001*\"femal\" + 0.001*\"brother\" + 0.001*\"tongu\" + 0.001*\"colleg\" + 0.001*\"topic\" + 0.001*\"class\" + 0.001*\"term\" + 0.001*\"creativ\" + 0.001*\"male\" + 0.001*\"sleep\"\n",
      "Topic: 42 Word: 0.001*\"intuit\" + 0.001*\"dont\" + 0.001*\"stereotyp\" + 0.001*\"domin\" + 0.001*\"connect\" + 0.001*\"user\" + 0.001*\"iphon\" + 0.001*\"extrovert\" + 0.001*\"enneagram\" + 0.001*\"charact\"\n",
      "Topic: 43 Word: 0.002*\"dont\" + 0.001*\"domin\" + 0.001*\"dear\" + 0.001*\"drink\" + 0.001*\"depress\" + 0.001*\"send\" + 0.001*\"medit\" + 0.001*\"messag\" + 0.001*\"suggest\" + 0.001*\"suck\"\n",
      "Topic: 44 Word: 0.001*\"hello\" + 0.001*\"user\" + 0.001*\"dont\" + 0.001*\"fuck\" + 0.001*\"logic\" + 0.001*\"dear\" + 0.001*\"drink\" + 0.001*\"intuit\" + 0.001*\"move\" + 0.001*\"stereotyp\"\n",
      "Topic: 45 Word: 0.001*\"tapatalk\" + 0.001*\"enneagram\" + 0.001*\"repli\" + 0.001*\"marri\" + 0.001*\"logic\" + 0.001*\"male\" + 0.001*\"alot\" + 0.001*\"descript\" + 0.001*\"hello\" + 0.001*\"eye\"\n",
      "Topic: 46 Word: 0.001*\"wink\" + 0.001*\"tongu\" + 0.001*\"dont\" + 0.001*\"penguin\" + 0.001*\"societi\" + 0.001*\"hello\" + 0.001*\"fuck\" + 0.001*\"drink\" + 0.001*\"draw\" + 0.001*\"annoy\"\n",
      "Topic: 47 Word: 0.002*\"dont\" + 0.001*\"sleep\" + 0.001*\"truli\" + 0.001*\"sister\" + 0.001*\"trust\" + 0.001*\"hello\" + 0.001*\"hurt\" + 0.001*\"wear\" + 0.001*\"valu\" + 0.001*\"struggl\"\n",
      "Topic: 48 Word: 0.001*\"tongu\" + 0.001*\"tapatalk\" + 0.001*\"send\" + 0.001*\"proud\" + 0.001*\"dear\" + 0.001*\"random\" + 0.001*\"parent\" + 0.001*\"sleep\" + 0.001*\"curious\" + 0.001*\"stress\"\n",
      "Topic: 49 Word: 0.001*\"fuck\" + 0.001*\"okay\" + 0.001*\"tapatalk\" + 0.001*\"hello\" + 0.001*\"cold\" + 0.001*\"mention\" + 0.001*\"insid\" + 0.001*\"proud\" + 0.001*\"hurt\" + 0.001*\"clear\"\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.40, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=50, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = \"tfidf_LDA_50_model\"\n",
    "lda_model_tfidf.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.001*\"result\" + 0.001*\"sleep\" + 0.001*\"hello\" + 0.001*\"class\" + 0.001*\"interview\" + 0.001*\"femal\" + 0.001*\"okay\" + 0.001*\"valu\" + 0.001*\"joke\" + 0.001*\"sister\"\n",
      "Topic: 1 Word: 0.002*\"dear\" + 0.001*\"dont\" + 0.001*\"fear\" + 0.001*\"beauti\" + 0.001*\"hurt\" + 0.001*\"myer\" + 0.001*\"brigg\" + 0.001*\"kinda\" + 0.001*\"food\" + 0.001*\"fuck\"\n",
      "Topic: 2 Word: 0.001*\"blah\" + 0.001*\"dont\" + 0.001*\"boyfriend\" + 0.001*\"sleep\" + 0.001*\"extrovert\" + 0.001*\"strang\" + 0.001*\"tongu\" + 0.001*\"wors\" + 0.001*\"class\" + 0.001*\"descript\"\n",
      "Topic: 3 Word: 0.001*\"dear\" + 0.001*\"fuck\" + 0.001*\"smile\" + 0.001*\"cute\" + 0.001*\"dont\" + 0.001*\"text\" + 0.001*\"state\" + 0.001*\"theori\" + 0.001*\"black\" + 0.001*\"socion\"\n",
      "Topic: 4 Word: 0.001*\"dont\" + 0.001*\"extrovert\" + 0.001*\"husband\" + 0.001*\"religion\" + 0.001*\"marri\" + 0.001*\"intuit\" + 0.001*\"money\" + 0.001*\"fuck\" + 0.001*\"math\" + 0.001*\"studi\"\n",
      "Topic: 5 Word: 0.002*\"dont\" + 0.001*\"charact\" + 0.001*\"enneagram\" + 0.001*\"commit\" + 0.001*\"marri\" + 0.001*\"topic\" + 0.001*\"instinct\" + 0.001*\"theori\" + 0.001*\"women\" + 0.001*\"half\"\n",
      "Topic: 6 Word: 0.001*\"eye\" + 0.001*\"beauti\" + 0.001*\"parent\" + 0.001*\"dont\" + 0.001*\"sim\" + 0.001*\"develop\" + 0.001*\"control\" + 0.001*\"walk\" + 0.001*\"lack\" + 0.001*\"super\"\n",
      "Topic: 7 Word: 0.001*\"send\" + 0.001*\"dont\" + 0.001*\"shit\" + 0.001*\"anim\" + 0.001*\"fuck\" + 0.001*\"note\" + 0.001*\"appreci\" + 0.001*\"tapatalk\" + 0.001*\"topic\" + 0.001*\"engin\"\n",
      "Topic: 8 Word: 0.001*\"hello\" + 0.001*\"intuit\" + 0.001*\"mother\" + 0.001*\"femal\" + 0.001*\"logic\" + 0.001*\"tongu\" + 0.001*\"connect\" + 0.001*\"suggest\" + 0.001*\"send\" + 0.001*\"fear\"\n",
      "Topic: 9 Word: 0.001*\"dont\" + 0.001*\"dear\" + 0.001*\"hello\" + 0.001*\"depress\" + 0.001*\"tapatalk\" + 0.001*\"enneagram\" + 0.001*\"brain\" + 0.001*\"women\" + 0.001*\"hell\" + 0.001*\"charact\"\n",
      "Topic: 10 Word: 0.001*\"quot\" + 0.001*\"fuck\" + 0.001*\"hurt\" + 0.001*\"number\" + 0.001*\"brother\" + 0.001*\"annoy\" + 0.001*\"depress\" + 0.001*\"difficult\" + 0.001*\"control\" + 0.001*\"rare\"\n",
      "Topic: 11 Word: 0.001*\"dont\" + 0.001*\"hurt\" + 0.001*\"result\" + 0.001*\"tongu\" + 0.001*\"kitteh\" + 0.001*\"stupid\" + 0.001*\"marriag\" + 0.001*\"parent\" + 0.001*\"studi\" + 0.001*\"brother\"\n",
      "Topic: 12 Word: 0.001*\"hello\" + 0.001*\"dont\" + 0.001*\"advic\" + 0.001*\"rock\" + 0.001*\"passion\" + 0.001*\"join\" + 0.001*\"sleep\" + 0.001*\"onlin\" + 0.001*\"rare\" + 0.001*\"perc\"\n",
      "Topic: 13 Word: 0.001*\"enneagram\" + 0.001*\"tongu\" + 0.001*\"femal\" + 0.001*\"fuck\" + 0.001*\"dont\" + 0.001*\"girlfriend\" + 0.001*\"obvious\" + 0.001*\"hello\" + 0.001*\"kitteh\" + 0.001*\"studi\"\n",
      "Topic: 14 Word: 0.001*\"fuck\" + 0.001*\"cri\" + 0.001*\"enneagram\" + 0.001*\"realiti\" + 0.001*\"brother\" + 0.001*\"dont\" + 0.001*\"accept\" + 0.001*\"okay\" + 0.001*\"blue\" + 0.001*\"hurt\"\n",
      "Topic: 15 Word: 0.002*\"tapatalk\" + 0.002*\"send\" + 0.001*\"iphon\" + 0.001*\"valu\" + 0.001*\"dont\" + 0.001*\"bodi\" + 0.001*\"light\" + 0.001*\"fuck\" + 0.001*\"charact\" + 0.001*\"socion\"\n",
      "Topic: 16 Word: 0.003*\"tmlt\" + 0.001*\"boyfriend\" + 0.001*\"beauti\" + 0.001*\"drink\" + 0.001*\"proud\" + 0.001*\"cognit\" + 0.001*\"hair\" + 0.001*\"tapatalk\" + 0.001*\"cute\" + 0.001*\"studi\"\n",
      "Topic: 17 Word: 0.001*\"tongu\" + 0.001*\"lack\" + 0.001*\"shit\" + 0.001*\"curious\" + 0.001*\"color\" + 0.001*\"easi\" + 0.001*\"women\" + 0.001*\"drink\" + 0.001*\"awar\" + 0.001*\"control\"\n",
      "Topic: 18 Word: 0.002*\"dont\" + 0.001*\"dear\" + 0.001*\"fuck\" + 0.001*\"femal\" + 0.001*\"hug\" + 0.001*\"perc\" + 0.001*\"hello\" + 0.001*\"sleep\" + 0.001*\"brother\" + 0.001*\"grade\"\n",
      "Topic: 19 Word: 0.001*\"shit\" + 0.001*\"teacher\" + 0.001*\"perc\" + 0.001*\"math\" + 0.001*\"research\" + 0.001*\"fuck\" + 0.001*\"kinda\" + 0.001*\"necessari\" + 0.001*\"annoy\" + 0.001*\"engin\"\n",
      "Topic: 20 Word: 0.001*\"dont\" + 0.001*\"hello\" + 0.001*\"kinda\" + 0.001*\"afraid\" + 0.001*\"enneagram\" + 0.001*\"male\" + 0.001*\"debat\" + 0.001*\"tongu\" + 0.001*\"dead\" + 0.001*\"constant\"\n",
      "Topic: 21 Word: 0.001*\"dont\" + 0.001*\"fiction\" + 0.001*\"exist\" + 0.001*\"worri\" + 0.001*\"studi\" + 0.001*\"kill\" + 0.001*\"score\" + 0.001*\"soul\" + 0.001*\"clear\" + 0.001*\"shit\"\n",
      "Topic: 22 Word: 0.001*\"fuck\" + 0.001*\"intuit\" + 0.001*\"dear\" + 0.001*\"drink\" + 0.001*\"brain\" + 0.001*\"shit\" + 0.001*\"okay\" + 0.001*\"dont\" + 0.001*\"choos\" + 0.001*\"questionnair\"\n",
      "Topic: 23 Word: 0.001*\"enneagram\" + 0.001*\"dont\" + 0.001*\"shit\" + 0.001*\"anim\" + 0.001*\"film\" + 0.001*\"studi\" + 0.001*\"descript\" + 0.001*\"okay\" + 0.001*\"star\" + 0.001*\"countri\"\n",
      "Topic: 24 Word: 0.001*\"dont\" + 0.001*\"heart\" + 0.001*\"quot\" + 0.001*\"enneagram\" + 0.001*\"stand\" + 0.001*\"fuck\" + 0.001*\"clear\" + 0.001*\"tongu\" + 0.001*\"anyway\" + 0.001*\"final\"\n",
      "Topic: 25 Word: 0.002*\"socion\" + 0.001*\"okay\" + 0.001*\"theori\" + 0.001*\"couldn\" + 0.001*\"tongu\" + 0.001*\"anim\" + 0.001*\"hello\" + 0.001*\"sister\" + 0.001*\"anxieti\" + 0.001*\"normal\"\n",
      "Topic: 26 Word: 0.001*\"femal\" + 0.001*\"repli\" + 0.001*\"male\" + 0.001*\"shit\" + 0.001*\"beauti\" + 0.001*\"obsess\" + 0.001*\"control\" + 0.001*\"gonna\" + 0.001*\"women\" + 0.001*\"hello\"\n",
      "Topic: 27 Word: 0.001*\"fuck\" + 0.001*\"dont\" + 0.001*\"danc\" + 0.001*\"studi\" + 0.001*\"food\" + 0.001*\"metal\" + 0.001*\"cognit\" + 0.001*\"brain\" + 0.001*\"class\" + 0.001*\"insid\"\n",
      "Topic: 28 Word: 0.001*\"enneagram\" + 0.001*\"tmlt\" + 0.001*\"tongu\" + 0.001*\"cognit\" + 0.001*\"extrovert\" + 0.001*\"blush\" + 0.001*\"wink\" + 0.001*\"dont\" + 0.001*\"parent\" + 0.001*\"fall\"\n",
      "Topic: 29 Word: 0.001*\"intuit\" + 0.001*\"enneagram\" + 0.001*\"anim\" + 0.001*\"exist\" + 0.001*\"theori\" + 0.001*\"perc\" + 0.001*\"evil\" + 0.001*\"logic\" + 0.001*\"allow\" + 0.001*\"avoid\"\n",
      "Topic: 30 Word: 0.001*\"dont\" + 0.001*\"hello\" + 0.001*\"logic\" + 0.001*\"repli\" + 0.001*\"shit\" + 0.001*\"english\" + 0.001*\"kinda\" + 0.001*\"fuck\" + 0.001*\"tongu\" + 0.001*\"wink\"\n",
      "Topic: 31 Word: 0.001*\"charact\" + 0.001*\"beauti\" + 0.001*\"proud\" + 0.001*\"inferior\" + 0.001*\"anim\" + 0.001*\"send\" + 0.001*\"fear\" + 0.001*\"truli\" + 0.001*\"ravenclaw\" + 0.001*\"tongu\"\n",
      "Topic: 32 Word: 0.002*\"dont\" + 0.002*\"youtub\" + 0.001*\"tongu\" + 0.001*\"seri\" + 0.001*\"dear\" + 0.001*\"lead\" + 0.001*\"kinda\" + 0.001*\"hello\" + 0.001*\"trityp\" + 0.001*\"kitteh\"\n",
      "Topic: 33 Word: 0.001*\"dont\" + 0.001*\"fuck\" + 0.001*\"enneagram\" + 0.001*\"depress\" + 0.001*\"babi\" + 0.001*\"proud\" + 0.001*\"brother\" + 0.001*\"okay\" + 0.001*\"domin\" + 0.001*\"anim\"\n",
      "Topic: 34 Word: 0.001*\"tongu\" + 0.001*\"dont\" + 0.001*\"clear\" + 0.001*\"ignor\" + 0.001*\"brother\" + 0.001*\"hello\" + 0.001*\"state\" + 0.001*\"studi\" + 0.001*\"way\" + 0.001*\"blush\"\n",
      "Topic: 35 Word: 0.002*\"dont\" + 0.001*\"flickr\" + 0.001*\"fuck\" + 0.001*\"extrovert\" + 0.001*\"scienc\" + 0.001*\"colleg\" + 0.001*\"class\" + 0.001*\"tongu\" + 0.001*\"shit\" + 0.001*\"sociopath\"\n",
      "Topic: 36 Word: 0.001*\"dont\" + 0.001*\"sleep\" + 0.001*\"drink\" + 0.001*\"math\" + 0.001*\"kinda\" + 0.001*\"shock\" + 0.001*\"beauti\" + 0.001*\"bear\" + 0.001*\"result\" + 0.001*\"hello\"\n",
      "Topic: 37 Word: 0.005*\"tapatalk\" + 0.004*\"iphon\" + 0.003*\"send\" + 0.001*\"dont\" + 0.001*\"drink\" + 0.001*\"intuit\" + 0.001*\"sister\" + 0.001*\"rule\" + 0.001*\"pictur\" + 0.001*\"wink\"\n",
      "Topic: 38 Word: 0.002*\"dont\" + 0.001*\"drink\" + 0.001*\"daughter\" + 0.001*\"bear\" + 0.001*\"hug\" + 0.001*\"respect\" + 0.001*\"logic\" + 0.001*\"mother\" + 0.001*\"gift\" + 0.001*\"discuss\"\n",
      "Topic: 39 Word: 0.001*\"dont\" + 0.001*\"critic\" + 0.001*\"scienc\" + 0.001*\"fuck\" + 0.001*\"charact\" + 0.001*\"connect\" + 0.001*\"femal\" + 0.001*\"child\" + 0.001*\"topic\" + 0.001*\"intellig\"\n",
      "Topic: 40 Word: 0.001*\"dont\" + 0.001*\"women\" + 0.001*\"sister\" + 0.001*\"trust\" + 0.001*\"depress\" + 0.001*\"tongu\" + 0.001*\"appreci\" + 0.001*\"domin\" + 0.001*\"result\" + 0.001*\"descript\"\n",
      "Topic: 41 Word: 0.001*\"femal\" + 0.001*\"brother\" + 0.001*\"tongu\" + 0.001*\"colleg\" + 0.001*\"topic\" + 0.001*\"class\" + 0.001*\"term\" + 0.001*\"creativ\" + 0.001*\"male\" + 0.001*\"sleep\"\n",
      "Topic: 42 Word: 0.001*\"intuit\" + 0.001*\"dont\" + 0.001*\"stereotyp\" + 0.001*\"domin\" + 0.001*\"connect\" + 0.001*\"user\" + 0.001*\"iphon\" + 0.001*\"extrovert\" + 0.001*\"enneagram\" + 0.001*\"charact\"\n",
      "Topic: 43 Word: 0.002*\"dont\" + 0.001*\"domin\" + 0.001*\"dear\" + 0.001*\"drink\" + 0.001*\"depress\" + 0.001*\"send\" + 0.001*\"medit\" + 0.001*\"messag\" + 0.001*\"suggest\" + 0.001*\"suck\"\n",
      "Topic: 44 Word: 0.001*\"hello\" + 0.001*\"user\" + 0.001*\"dont\" + 0.001*\"fuck\" + 0.001*\"logic\" + 0.001*\"dear\" + 0.001*\"drink\" + 0.001*\"intuit\" + 0.001*\"move\" + 0.001*\"stereotyp\"\n",
      "Topic: 45 Word: 0.001*\"tapatalk\" + 0.001*\"enneagram\" + 0.001*\"repli\" + 0.001*\"marri\" + 0.001*\"logic\" + 0.001*\"male\" + 0.001*\"alot\" + 0.001*\"descript\" + 0.001*\"hello\" + 0.001*\"eye\"\n",
      "Topic: 46 Word: 0.001*\"wink\" + 0.001*\"tongu\" + 0.001*\"dont\" + 0.001*\"penguin\" + 0.001*\"societi\" + 0.001*\"hello\" + 0.001*\"fuck\" + 0.001*\"drink\" + 0.001*\"draw\" + 0.001*\"annoy\"\n",
      "Topic: 47 Word: 0.002*\"dont\" + 0.001*\"sleep\" + 0.001*\"truli\" + 0.001*\"sister\" + 0.001*\"trust\" + 0.001*\"hello\" + 0.001*\"hurt\" + 0.001*\"wear\" + 0.001*\"valu\" + 0.001*\"struggl\"\n",
      "Topic: 48 Word: 0.001*\"tongu\" + 0.001*\"tapatalk\" + 0.001*\"send\" + 0.001*\"proud\" + 0.001*\"dear\" + 0.001*\"random\" + 0.001*\"parent\" + 0.001*\"sleep\" + 0.001*\"curious\" + 0.001*\"stress\"\n",
      "Topic: 49 Word: 0.001*\"fuck\" + 0.001*\"okay\" + 0.001*\"tapatalk\" + 0.001*\"hello\" + 0.001*\"cold\" + 0.001*\"mention\" + 0.001*\"insid\" + 0.001*\"proud\" + 0.001*\"hurt\" + 0.001*\"clear\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "# data = datapath(\"my_LDA_model\")\n",
    "model_test = gensim.models.LdaMulticore.load(\"tfidf_LDA_50_model\")\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YO\n"
     ]
    }
   ],
   "source": [
    "all_data_test = []\n",
    "\n",
    "def read_lines_test():\n",
    "\twith open('../data/test_data.csv') as csv_file:\n",
    "\t\tcsv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\t\tline_count = 0\n",
    "\t\tprint(\"YO\")\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif (line_count != 0):\n",
    "\t\t\t\tall_data_test.append(row)\n",
    "\t\t\tline_count += 1\n",
    "read_lines_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('lda_results_train_50.csv', mode='w') as write_file:\n",
    "    employee_writer = csv.writer(write_file, delimiter=',')\n",
    "#     employee_writer.writerow(['mbti', 'text', 'sadness', 'joy', 'fear', 'disgust', 'anger', 'topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14', 'topic15'])\n",
    "    for row in all_data:\n",
    "        doc = row[1]\n",
    "        processed_test_doc = preprocess(doc)\n",
    "        clean_doc = dictionary.doc2bow(processed_test_doc)\n",
    "    #     v = corpus_tfidf[clean_docs]\n",
    "        vect = lda_model_tfidf[clean_doc]\n",
    "        new_row = []\n",
    "        new_row.append(int(row[0]))\n",
    "        new_row.append(row[1])\n",
    "        new_row.append(float(row[2]))\n",
    "        new_row.append(float(row[3]))\n",
    "        new_row.append(float(row[4]))\n",
    "        new_row.append(float(row[5]))\n",
    "        new_row.append(float(row[6]))\n",
    "        all_scores = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        for topic_index, score in vect:\n",
    "            all_scores[topic_index] = score\n",
    "        new_row += all_scores\n",
    "        employee_writer.writerow(new_row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('lda_results_test_50.csv', mode='w') as write_file:\n",
    "    employee_writer = csv.writer(write_file, delimiter=',')\n",
    "#     employee_writer.writerow(['mbti', 'text', 'sadness', 'joy', 'fear', 'disgust', 'anger', 'topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14', 'topic15'])\n",
    "    for row in all_data_test:\n",
    "        doc = row[1]\n",
    "        processed_test_doc = preprocess(doc)\n",
    "        clean_doc = dictionary.doc2bow(processed_test_doc)\n",
    "    #     v = corpus_tfidf[clean_docs]\n",
    "        vect = lda_model_tfidf[clean_doc]\n",
    "        new_row = []\n",
    "        new_row.append(int(row[0]))\n",
    "        new_row.append(row[1])\n",
    "        new_row.append(float(row[2]))\n",
    "        new_row.append(float(row[3]))\n",
    "        new_row.append(float(row[4]))\n",
    "        new_row.append(float(row[5]))\n",
    "        new_row.append(float(row[6]))\n",
    "        all_scores = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        for topic_index, score in vect:\n",
    "            all_scores[topic_index] = score\n",
    "        new_row += all_scores\n",
    "        employee_writer.writerow(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = \"tfidf_LDA_model\"\n",
    "lda_model_tfidf.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
